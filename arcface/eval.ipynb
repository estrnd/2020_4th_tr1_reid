{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "honey-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency package import\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sharp-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 이미지 리스트\n",
    "BASE_DIR = '/data/kts123/aihub/reid'\n",
    "test_imgs = '/data/kts123/aihub/reid/img_list_test.txt'\n",
    "\n",
    "# 훈련된 모델경로\n",
    "weight_path = '/home/kts123/gc2021/3차/track3/arcface/checkpoints_res50_base/scheduler_resnet50_99.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "agreed-meaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef601b52a034e9fb2057c9e912b1ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(test_imgs)\n",
    "for kls, df_i in tqdm(df.groupby('KLS_IDX')):\n",
    "    dst_dir = f'eval_imgs/{kls:05d}'\n",
    "    Path(dst_dir).mkdir(exist_ok=True, parents=True)\n",
    "    for name in df_i['NAME'].values:\n",
    "        src = f'{BASE_DIR}/{name}'\n",
    "        dst = f'{dst_dir}/{Path(name).name}'\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "quick-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, root_dir, trsf):\n",
    "        \n",
    "        to_pname = lambda x : Path(x).parent.name\n",
    "        \n",
    "        imgs = sorted(glob(f'{root_dir}/*/*.jpg'))\n",
    "        pnames = list({to_pname(e) for e in imgs})\n",
    "        \n",
    "        to_label = lambda x: pnames.index(to_pname(x)) \n",
    "        self.imgs = [[to_label(e), e] for e in tqdm(imgs)]\n",
    "        self.trsf = trsf \n",
    "        self.num_labels = len(pnames) \n",
    "    \n",
    "    def select_random(self, idx, is_positive=True):\n",
    "        pivot_label, pivot_path = self.imgs[idx]\n",
    "        while True:\n",
    "            label, path = random.choice(self.imgs)\n",
    "            if path == pivot_path:\n",
    "                continue\n",
    "            if (label == pivot_label) is is_positive:\n",
    "                return path\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, pivot_path = self.imgs[idx]\n",
    "        pos_path = self.select_random(idx, is_positive=True)\n",
    "        neg_path = self.select_random(idx, is_positive=False)\n",
    "        paths = [pivot_path, pos_path, neg_path]\n",
    "        imgs = [Image.open(e) for e in paths]\n",
    "        imgs = [self.trsf(e) for e in imgs]\n",
    "        return imgs, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms =  transforms.Compose([\n",
    "        transforms.Resize(int(IMG_SIZE*1.2)),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  EvalDataset(CROP_IMG_DIR, data_transforms) \n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "print(len(dataset), dataset.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "original-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricModel(nn.Module):\n",
    "    def __init__(self, weights_path, device):\n",
    "        super(MetricModel, self).__init__()\n",
    "        \n",
    "        backbone = models.resnext50_32x4d(pretrained=True)\n",
    "        backbone.fc = nn.Linear(in_features=backbone.fc.in_features, out_features=METRIC_DIM)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        weights = torch.load(weights_path)\n",
    "        self.device = device\n",
    "        self.backbone.load_state_dict(weights)\n",
    "        self.to(device)\n",
    "        self.eval() \n",
    "        \n",
    "    def forward(self, input_imgs):\n",
    "        with torch.no_grad():\n",
    "            x = self.backbone(input_imgs)\n",
    "            x = F.normalize(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-nashville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc21_3_3]",
   "language": "python",
   "name": "conda-env-gc21_3_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
